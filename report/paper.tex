\documentclass[conference]{IEEEtran}

% *** PACKAGES ***
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{cite}
\usepackage{subcaption}
\usepackage{stfloats} % better control for double-column floats

% Slightly more open, readable layout
\setlength{\parskip}{0.5ex}
\setlength{\parindent}{1em}
\linespread{1.03}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue
}

% Path to plots (relative to report directory)
\graphicspath{{../plots/}}

\begin{document}

\title{Machine Learning for Network Traffic Prediction and Capacity Planning}

\author{
\IEEEauthorblockN{Suryansh Singh Sijwali}
\IEEEauthorblockA{
Dept. of Computer Science\\
Penn State University\\
University Park, PA, USA\\
sss6371@psu.edu}
}

\maketitle

\begin{abstract}
Accurate prediction of network traffic is a core enabler for intelligent capacity planning in backbone networks. Overestimating demand leads to expensive over-provisioning, while underestimating it causes congestion and packet loss. This paper studies how forecasting quality translates into capacity planning decisions by comparing a traditional statistical model---Seasonal ARIMA (SARIMA)---with a deep learning model---Long Short-Term Memory (LSTM)---on a synthetic but realistic backbone network. I design a complete simulation pipeline that generates multi-day traffic traces with diurnal patterns and burstiness on a 12-node topology, trains SARIMA per link and an LSTM jointly over all links, evaluates forecasting quality using RMSE, MAE, and MAPE, and finally quantifies capacity planning outcomes such as maximum link utilization and overload fraction. The results show that LSTM reduces RMSE by approximately 35\% relative to SARIMA and cuts overload events by a factor of roughly four at similar capacity margins, demonstrating that modern sequence models can directly improve network engineering decisions and not just prediction accuracy.
\end{abstract}

\begin{IEEEkeywords}
Network traffic prediction, capacity planning, SARIMA, LSTM, time series forecasting, backbone networks.
\end{IEEEkeywords}

%=========================================================
\section{Introduction}
%=========================================================

Modern communication networks carry highly variable traffic with strong temporal and spatial structure. Operators must determine how much capacity to deploy on each link to accommodate peak loads while keeping cost under control. Traditionally, capacity planning relies on heuristics, long-term averages, and conservative safety margins. However, as traffic patterns become more complex and applications more latency-sensitive, \emph{data-driven} forecasting is increasingly important for making principled planning decisions rather than just ``best guesses.''

Network traffic exhibits well-known features such as diurnal cycles, weekly periodicity, and heavy-tailed bursts.\cite{papagiannaki2003long} These properties motivate both classical time series models (e.g., ARIMA/SARIMA) and modern deep learning models (e.g., LSTM, graph neural networks) for traffic prediction.\cite{box2015time,hochreiter1997lstm,abbasi2021ntma} From a statistics point of view, traffic prediction is a fairly standard forecasting problem; from a networking point of view, small differences in error can translate into very different engineering decisions.

In this project I wanted to connect those two perspectives: not just ``which model has lower RMSE,'' but ``does that actually change how much capacity I would provision on a backbone link?'' To explore that question in a controlled setting, I built a Python simulation pipeline that generates synthetic traffic on a small backbone topology, trains both SARIMA and LSTM forecasts, and feeds the resulting predictions into a simple capacity planning rule. The complete, reproducible codebase is publicly available.\footnote{Repository: \url{https://github.com/SuryanshSS1011/ML-for-Network-Traffic-Prediction-and-Capacity-Planning}}

\subsection{Problem formulation and assumptions}

I consider a backbone network with $L$ links. At discrete times $t = 1, 2, \dots$, we observe a vector of link loads
\[
\mathbf{y}_t \in \mathbb{R}^L_{\ge 0},
\]
where $y_{\ell,t}$ is the aggregate load on link $\ell$ during interval $t$. Given a history of past loads, the goal is to predict future loads:
\[
\hat{\mathbf{y}}_{t+1} = f_\theta(\mathbf{y}_t, \mathbf{y}_{t-1}, \dots, \mathbf{y}_{t-W+1}),
\]
where $W$ is the input window size and $f_\theta$ is a forecasting model with parameters~$\theta$.

Throughout this paper, I make the following assumptions:
\begin{itemize}
    \item \textbf{Regularity.} Traffic exhibits approximate daily structure and slow evolution over the 14-day horizon.
    \item \textbf{Measurement availability.} Per-link load measurements are available at a fixed sampling interval (5 minutes).
    \item \textbf{Fixed topology.} The network topology and routing do not change over the forecasting window.
    \item \textbf{Single-step horizon.} I focus on one-step-ahead prediction at a 5-minute resolution.
\end{itemize}

I compare:
\begin{itemize}
    \item a per-link SARIMA model, treating each $y_{\ell,t}$ as an independent univariate series;
    \item a joint LSTM model, trained across links.
\end{itemize}

%=========================================================
\section{Classification of ML Approaches for Traffic Prediction}
%=========================================================

This section classifies existing work on ML for traffic prediction, setting up the categories used in the rest of the paper and giving some intuition on where different methods fit.

\subsection{Model-based vs.\ data-driven axes}

Two high-level axes are useful when zooming out over the literature.

\subsubsection{Modeling paradigm}

The first axis concerns \emph{how} traffic is modeled:

\begin{itemize}
    \item \textbf{Classical statistical and shallow ML models} (e.g., ARIMA, SARIMA, VAR, SVR): emphasize linear structure, interpretable parameters, and simple feature sets.\cite{box2015time,ferreira2022forecasting}
    \item \textbf{Deep sequence models} (e.g., LSTM, GRU, temporal CNNs): learn complex nonlinear temporal dependencies directly from data.\cite{hochreiter1997lstm,abbasi2021ntma}
    \item \textbf{Graph-based spatio-temporal models} (GNNs): explicitly incorporate network topology as a graph, combining message passing and temporal modeling.\cite{wu2020gnnsurvey,sun2025cchebnet}
    \item \textbf{Data-centric and hybrid approaches}: focus on data representation, missing data, and combinations such as ARIMA + deep residuals.\cite{hu2019dclstm,zhang2024images}
\end{itemize}

\subsubsection{Network context and prediction target}

The second axis concerns \emph{what} is being predicted and in which environment:

\begin{itemize}
    \item \textbf{Context:} backbone/WANs, datacenters, access networks, or cellular systems.\cite{abbasi2021ntma,chen2021survey}
    \item \textbf{Target:} link loads, traffic matrices (TMs), or flow-level metrics such as flow completion times.
\end{itemize}

Backbone and WAN settings often emphasize TMs and link loads; cellular work often focuses on per-cell load or user demand; datacenter studies may look at flow-level or rack-level metrics.

\subsection{Evaluation metrics and pitfalls}

Across these contexts, evaluation most often uses RMSE, MAE, and MAPE.\cite{ferreira2022forecasting} A few practical issues show up repeatedly:

\begin{itemize}
    \item \textbf{MAPE on small values.} When $y_i$ is close to zero, MAPE can explode; many works add a small $\epsilon$ or report MAPE only above a threshold.
    \item \textbf{Per-link vs.\ global metrics.} Global averages can hide large errors on individual links; per-link distributions are important for network engineers.
    \item \textbf{Train/test leakage.} Using future samples in normalization, or tuning hyperparameters on test sets, is surprisingly common in older work.
    \item \textbf{Operational relevance.} A model with slightly lower RMSE is not necessarily better if it leads to worse decisions downstream (e.g., unstable capacity plans).
\end{itemize}

These issues motivated the design of the experiment in Section~\ref{sec:perf}: look at both error metrics and how those errors propagate into a simple capacity planning decision.

%=========================================================
\section{Modeling Categories: Details, Pros, and Cons}
%=========================================================

This section presents each modeling category in more detail, along with typical use cases, advantages, and limitations.

\subsection{Classical statistical and shallow ML}

Classical models such as ARIMA and SARIMA have been widely applied to network traffic.\cite{papagiannaki2003long,box2015time} After differencing, the residual time series is assumed to be approximately stationary and modeled as a linear combination of past values and past noise.

\textbf{Typical use.} Per-link or aggregate traffic prediction in backbones and access networks; quick forecasting for capacity reporting and trend analysis.

\textbf{Extensions.} Multivariate time series models like vector autoregression (VAR) and state-space models can capture interactions between series, but they scale poorly as the number of links grows.\cite{ferreira2022forecasting}

\textbf{Pros:}
\begin{itemize}
    \item Interpretable coefficients and clear statistical assumptions.
    \item Fast to fit and diagnose; well-tested tooling.
    \item Strong performance on smooth, strongly seasonal traffic.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item Limited ability to capture nonlinear dynamics or sharp bursts.
    \item Usually treat each link independently, missing cross-link correlations.
    \item Order selection and seasonal structure must be chosen by the user or tuned per series.
\end{itemize}

\textbf{Best suited for:} relatively stable enterprise or backbone networks with clear seasonality and limited burstiness, where interpretability and speed are more important than squeezing out the last few percent of accuracy.

Shallow ML methods (SVR, random forests, gradient boosting) have also been applied.\cite{ferreira2022forecasting} They often use hand-crafted features (lags, moving averages, time-of-day indicators) and can be seen as a middle ground between purely linear models and deep learning.

\subsection{Deep sequence models}

Deep sequence models treat traffic as a multivariate time series and use architectures such as LSTMs, GRUs, or temporal CNNs.\cite{hochreiter1997lstm,abbasi2021ntma,chen2021survey}

Aloraifan \emph{et al.}\cite{he2017tmdldeep} use deep neural networks to predict traffic matrices in large IP backbones, reporting significant improvements over ARIMA-style baselines. Hu \emph{et al.}\cite{hu2019dclstm} propose a deep convolutional LSTM network for TM prediction with partial information, mixing spatial and temporal extraction.

Beyond these individual examples, several recent works apply deep architectures to traffic matrix prediction and network optimization in a variety of settings. Nie \emph{et al.}\cite{nie2016traffic} introduce deep learning for TM prediction in large-scale IP backbone networks. Troia \emph{et al.}\cite{polimi2020dltpno} explore deep learning-based traffic prediction as a building block for network optimization in optical networks, while Lai \emph{et al.}\cite{zhang2021deeptm} develop a deep learning-based traffic prediction method for digital twin networks. Zhao \emph{et al.}\cite{zhao2023nnotm} apply neural traffic-matrix prediction to optical network-on-chip (ONoC), showing how similar ideas carry over to on-chip interconnects.

\textbf{Pros:}
\begin{itemize}
    \item Capture nonlinear temporal dependencies and complex interactions between flows and links.
    \item Naturally handle multi-step outputs and multiple covariates.
    \item Often significantly outperform classical models on complex or bursty traffic.\cite{ferreira2022forecasting}
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item Require more data and careful hyperparameter tuning (learning rate, hidden size, regularization).
    \item Harder to interpret; model decisions are often opaque.
    \item Training and inference are more computationally intensive than fitting an ARIMA per link.
\end{itemize}

\textbf{Best suited for:} backbone, cellular, or datacenter environments where long traffic histories are available, traffic exhibits strong nonlinearity or bursts, and the operator is willing to deploy and maintain ML models.

\subsection{Graph-based spatio-temporal models}

Graph neural networks (GNNs) explicitly incorporate network topology.\cite{wu2020gnnsurvey,sun2025cchebnet} Nodes represent routers, PoPs, or regions; edges represent links. Spatio-temporal architectures apply graph convolutions at each timestep, then feed node embeddings through temporal modules.

Sun \emph{et al.}\cite{sun2025cchebnet} use a ChebNet-based architecture for traffic flow prediction with non-local spatial correlations. Similar ideas have been applied to transportation, cellular networks, and IP networks.

\textbf{Pros:}
\begin{itemize}
    \item Capture spatial correlations and propagation of congestion naturally.
    \item Share information across nearby nodes/links in a structured way.
    \item Can encode topology changes explicitly.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item More complex to implement, tune, and debug.
    \item Require a reasonably accurate and stable graph representation.
    \item Benchmarking and reproducibility are harder than with standard sequence models.
\end{itemize}

\textbf{Best suited for:} settings where topology effects are important (e.g., rerouting, link failures, spatially correlated bursts), and where operators already have good graph abstractions of their networks.

\subsection{Data-centric and hybrid approaches}

Data-centric and hybrid approaches focus on representation, missing data, and combining different model types.\cite{ferreira2022forecasting,hu2019dclstm,zhang2024images}

Examples include:
\begin{itemize}
    \item Joint TM completion and prediction using deep architectures.\cite{hu2019dclstm}
    \item Treating traffic as images (e.g., time vs.\ OD pair) and applying CNNs.\cite{zhang2024images}
    \item Using deep models to learn residuals on top of ARIMA or Holtâ€“Winters.\cite{ferreira2022forecasting}
\end{itemize}

\textbf{Pros:}
\begin{itemize}
    \item Address practical issues such as missing counters and noisy measurements.
    \item Allow the combination of interpretable baselines and flexible deep models.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
    \item Pipelines can become complex, with multiple components to maintain.
    \item Reproducibility is harder; small implementation details matter.
\end{itemize}

\textbf{Best suited for:} real-world deployments where measurement gaps, noise, and changing instrumentation are the main pain points, and where incremental improvements over existing baselines are preferred over a full model replacement.

\medskip

In the rest of the paper, I zoom in on one specific comparison inside this landscape: a classical per-link SARIMA model versus a deep sequence LSTM model, and I evaluate not just forecasting error but also capacity planning outcomes.

%=========================================================
\section{Performance Comparison via Simulation}
\label{sec:perf}
%=========================================================

This section describes the simulation setup, forecasting models, evaluation metrics, and results. It corresponds to the ``performance comparison'' part of the assignment.

\subsection{Simulation setup and key parameters}

I simulate a backbone network with:
\begin{itemize}
    \item $N = 12$ nodes (points-of-presence),
    \item undirected edges representing bidirectional links.
\end{itemize}

The topology is generated with \texttt{networkx} and stored in a compact NumPy format (\texttt{data/topology.npz}). Each undirected edge is modeled as a single link time series representing aggregate load in both directions.

I simulate traffic for:
\begin{itemize}
    \item $T_{\text{days}} = 14$ days,
    \item sampling interval $\Delta t = \SI{5}{\minute}$,
\end{itemize}
yielding:
\[
T = 14 \times 24 \times \frac{60}{5} = 4032
\]
time steps per link.

Traffic on each link is generated via a simple model combining:
\begin{itemize}
    \item link-specific baseline $b_\ell$ and amplitude $A_\ell$,
    \item a smooth diurnal pattern $f_{\text{daily}}(t)$,
    \item Gaussian noise $\epsilon_\ell(t)$,
    \item occasional bursts $B_\ell(t)$.
\end{itemize}

I split the 14 days into 10 days for training and 4 days for evaluation. The key configuration parameters are summarized in Table~\ref{tab:config}; they are defined in a central configuration file (\texttt{config.py}) so that the entire pipeline is reproducible.

\begin{table}[h]
\centering
\caption{Simulation and Model Configuration}
\label{tab:config}
\begin{tabular}{l l}
\toprule
Parameter & Value / Description \\
\midrule
\texttt{num\_nodes} & 12 backbone nodes \\
\texttt{days} & 14 total days \\
\texttt{time\_step\_minutes} & 5-minute sampling \\
\texttt{window\_size} & 24 (2-hour LSTM input window) \\
\texttt{arima\_order} & $(2, 1, 2)$ \\
\texttt{seasonal\_order} & $(1, 0, 1, 72)$ (6-hour seasonality) \\
\texttt{capacity\_margin} & 1.10 (10\% safety margin) \\
LSTM hidden units & 64 \\
Optimizer & Adam \\
Loss & MSE (forecast vs.\ ground truth) \\
Train/eval split & 10 days train, 4 days eval \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Forecasting models}

\subsubsection{Per-link SARIMA}

For each link $\ell$, I fit a Seasonal ARIMA (SARIMA) model:
\[
\text{SARIMA}(p, d, q) \times (P, D, Q)_s,
\]
with:
\begin{itemize}
    \item $(p, d, q) = (2, 1, 2)$,
    \item $(P, D, Q, s) = (1, 0, 1, 72)$, corresponding to a \SI{6}{\hour} season at 5-minute resolution.
\end{itemize}

Model fitting is implemented using \texttt{statsmodels}. Each link is modeled independently; forecasts are produced for the 4-day evaluation period. I intentionally use a fixed order rather than \texttt{auto\_arima} to keep the baseline simple, reproducible, and fast.

\subsubsection{Joint LSTM forecaster}

The LSTM forecaster is trained jointly on all links. For each link and time, I construct input windows of length $W = 24$ (2 hours) and predict the next load:
\[
\hat{y}_\ell(t+1) = f_\theta(y_\ell(t-W+1),\dots,y_\ell(t)).
\]

The architecture is:
\begin{itemize}
    \item input shape $(\text{batch}, W, 1)$,
    \item one LSTM layer with 64 hidden units,
    \item a fully-connected layer mapping the last hidden state to a scalar.
\end{itemize}

I standardize each link using its training mean and standard deviation, train with MSE loss and the Adam optimizer, and select the best checkpoint based on validation error. The model is implemented in PyTorch and saved as \texttt{models/lstm\_forecaster.pt}.

\subsection{Evaluation metrics}

\subsubsection{Forecasting metrics}

I compute three standard error metrics across all forecasted points and links:

\textbf{Root Mean Squared Error (RMSE):}
\[
\text{RMSE} = \sqrt{\frac{1}{N_{\text{pts}}} \sum_{i=1}^{N_{\text{pts}}} (y_i - \hat{y}_i)^2}.
\]

\textbf{Mean Absolute Error (MAE):}
\[
\text{MAE} = \frac{1}{N_{\text{pts}}} \sum_{i=1}^{N_{\text{pts}}} |y_i - \hat{y}_i|.
\]

\textbf{Mean Absolute Percentage Error (MAPE):}
\[
\text{MAPE} = \frac{100}{N_{\text{pts}}} \sum_{i=1}^{N_{\text{pts}}} \left|\frac{y_i - \hat{y}_i}{y_i + \epsilon}\right|,
\]
where $\epsilon$ is a small constant to avoid division by zero.

\subsubsection{Capacity planning metrics}

To connect forecasting quality to capacity planning, I assume that each operator sets a fixed capacity $C_\ell$ over the evaluation period based on predicted peak load:
\[
\hat{y}_\ell^{\max} = \max_{t \in \text{eval}} \hat{y}_\ell(t),
\quad
C_\ell = \alpha \cdot \hat{y}_\ell^{\max},
\]
with safety margin $\alpha = 1.10$.

Given true loads $y_\ell(t)$ and capacities $C_\ell$, I define:
\[
u_\ell(t) = \frac{y_\ell(t)}{C_\ell},
\]
and measure:
\begin{itemize}
    \item maximum utilization per link:
    \[
    U_\ell^{\max} = \max_t u_\ell(t),
    \]
    \item overload fraction per link:
    \[
    f_\ell^{\text{over}} = \frac{1}{T_{\text{eval}}} \sum_{t} \mathbf{1}\{u_\ell(t) > 1\}.
    \]
\end{itemize}

I then average these over links to obtain ``Mean U\_max'' and ``Mean Overload.'' An oracle baseline sets
\[
C_\ell^{\text{oracle}} = \alpha \cdot \max_{t \in \text{eval}} y_\ell(t),
\]
representing perfect forecasting ability.

\subsection{Results: forecasting accuracy}

All steps are executed via:
\begin{verbatim}
python main.py
\end{verbatim}
which orchestrates data generation, model training, evaluation, and plotting using the configuration in Table~\ref{tab:config}.

Table~\ref{tab:forecast} summarizes global forecasting performance.

\begin{table}[h]
\centering
\caption{Global Forecasting Metrics (Lower is Better)}
\label{tab:forecast}
\begin{tabular}{lccc}
\toprule
Model & RMSE & MAE & MAPE \\
\midrule
SARIMA & 25.01 & 19.99 & 44.6\% \\
LSTM   & \textbf{16.31} & \textbf{12.08} & \textbf{25.4\%} \\
\bottomrule
\end{tabular}
\end{table}

The LSTM achieves a substantial reduction in error:
\begin{itemize}
    \item approximately 35\% lower RMSE than SARIMA,
    \item approximately 40\% lower MAPE.
\end{itemize}

\begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{forecast_rmse_mean.png}
        \caption{Mean RMSE}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{forecast_mae_mean.png}
        \caption{Mean MAE}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{forecast_mape_mean.png}
        \caption{Mean MAPE}
    \end{subfigure}
    \caption{Forecasting error metrics for SARIMA and LSTM, averaged over all links and evaluation timesteps.}
    \label{fig:forecast-bars}
\end{figure*}

To examine variability across links, Fig.~\ref{fig:rmse-hist} shows the distribution of per-link RMSE values. The LSTM curve is clearly shifted toward lower error bins: visually, a large majority of links see a reduction in RMSE under LSTM, with only a small minority where SARIMA is slightly better or roughly tied.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\columnwidth]{rmse_histogram.png}
    \caption{Histogram of per-link RMSE for SARIMA and LSTM. The LSTM shifts most links toward lower error.}
    \label{fig:rmse-hist}
\end{figure}

From a ``stats'' perspective, the takeaway is that using the same data and similar modeling cost per time step, a relatively small LSTM can substantially reduce error across multiple metrics and across most individual links, not just on average.

\subsection{Results: capacity planning outcomes}

The capacity planning metrics are given in Table~\ref{tab:capacity}. Fig.~\ref{fig:capacity-metrics} shows all three capacity-related metrics as a single three-panel figure for easier comparison.

\begin{table}[h]
\centering
\caption{Capacity Planning Metrics (Derived from Forecasts)}
\label{tab:capacity}
\begin{tabular}{lcc}
\toprule
Model  & Mean U\_max & Mean Overload \\
\midrule
SARIMA & 2.79    & 40.3\% \\
LSTM   & 1.94    & 10.9\% \\
Oracle & 1.59    & 2.4\%  \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{capacity_u_max_mean.png}
        \caption{Mean maximum utilization}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{capacity_f_over_mean.png}
        \caption{Mean overload fraction}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{capacity_u_max_max.png}
        \caption{Worst-case max utilization}
    \end{subfigure}
    \caption{Capacity planning metrics under SARIMA, LSTM, and Oracle capacities.}
    \label{fig:capacity-metrics}
\end{figure*}

Interpretation:
\begin{itemize}
    \item \textbf{SARIMA}: To avoid overloads, capacity must be set high relative to true peaks, resulting in average maximum utilizations close to 2.8 and frequent overload events (about 40\% of the evaluation period on average). The worst-case link experiences even higher peak utilization, as shown in Fig.~\ref{fig:capacity-metrics}c.
    \item \textbf{LSTM}: Better forecasts allow capacity to be closer to true demand. Maximum utilization drops to about 1.94 on average, with overloads reduced to around 11\%. Both the mean overload fraction and the worst-case utilization move closer to the oracle baseline.
    \item \textbf{Oracle}: With perfect knowledge of future peaks, both over-provisioning and overloads can be minimized; this provides a lower bound and shows remaining room for model improvement.
\end{itemize}

Looking at Fig.~\ref{fig:capacity-metrics} overall, the visual story matches the numbers: for every capacity-related metric, the LSTM sits noticeably closer to the oracle than SARIMA does.

\subsection{Representative time-series examples}

Finally, I inspect individual links to see when each model performs well. Fig.~\ref{fig:timeseries-examples} shows three representative cases.

\begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{timeseries_link_8_lstm_better.png}
        \caption{Link with strong LSTM advantage}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{timeseries_link_4_sarima_better.png}
        \caption{Link where SARIMA slightly wins}
    \end{subfigure}\hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{timeseries_link_18_median_perf.png}
        \caption{Link with median difficulty}
    \end{subfigure}
    \caption{Representative links illustrating model behavior across different traffic patterns.}
    \label{fig:timeseries-examples}
\end{figure*}

The left panel shows a link with pronounced peaks and bursts where LSTM tracks the true load much more closely than SARIMA. The middle panel shows a smoother link where SARIMA's linear structure is adequate and sometimes slightly better. The right panel is a ``typical'' link showing moderate but consistent LSTM improvements.

These examples support the idea that in a real deployment, operators might selectively apply deep models to more challenging links while leaving simple models on well-behaved ones.

\subsection{Discussion and link to the survey}

The experiment highlights several design trade-offs:

\begin{itemize}
    \item \textbf{Model complexity vs.\ robustness.} SARIMA is simple and interpretable but misses complex nonlinear effects; LSTM adapts better to bursts but needs more careful tuning.
    \item \textbf{Per-link vs.\ joint training.} The per-link SARIMA fits each series in isolation, while the LSTM jointly sees many different patterns. This joint training likely helps the LSTM generalize.
    \item \textbf{Error metrics vs.\ decisions.} A 30--40\% reduction in RMSE/MAPE can look abstract; seeing overload drop from 40\% to 11\% makes the engineering impact concrete.
\end{itemize}

Conceptually, the survey in Sections~2 and~3 motivated the choice of SARIMA and LSTM as representative classical and deep sequence models. The simulation in this section then concretely shows how their qualitative pros and cons play out in a capacity planning scenario, turning the literature discussion into a specific, measurable comparison.

%=========================================================
\section{Conclusion and Emerging Trends}
%=========================================================

This work builds an end-to-end simulation framework to study how forecasting models shape capacity planning in backbone networks. On a synthetic 12-node topology with 14 days of generated traffic, I compare SARIMA and LSTM forecasters and show that:

\begin{itemize}
    \item LSTM substantially reduces prediction errors (RMSE, MAE, MAPE) relative to SARIMA.
    \item These improvements translate into lower over-provisioning and significantly fewer overload events at the same safety margin.
    \item An oracle baseline shows further headroom, motivating more advanced models and topology-aware architectures.
\end{itemize}

From a broader perspective, the project reinforced a simple but important point: in network engineering, the value of a ``better'' model is only as real as the operational decisions it changes. By explicitly connecting forecasts to a capacity rule and then measuring overloads, it becomes much clearer why a 10--20\% difference in error can matter a lot in practice.

\subsection{Threats to validity and limitations}

There are several limitations and potential threats to validity:

\begin{itemize}
    \item \textbf{Synthetic traffic.} The experiments use synthetic traffic with hand-crafted diurnal patterns and bursts; real traces may exhibit different multi-scale dynamics, abrupt regime changes, or anomalies.
    \item \textbf{Single-step forecasting.} I only evaluate one-step-ahead predictions at 5-minute resolution. Multi-step forecasting (e.g., 1 hour ahead) may amplify error differences and change the relative ranking of models.
    \item \textbf{Single model configurations.} The study uses one fixed SARIMA order and a single LSTM architecture without extensive hyperparameter sweeps. Tuning either model could change absolute numbers and possibly narrow or widen the gap.
    \item \textbf{Single safety margin.} Capacity is derived from forecasts using a single safety margin $\alpha = 1.10$. In reality, operators might vary safety margins across links or use more complex cost/risk trade-offs.
\end{itemize}

These choices were deliberate to keep the project tractable and transparent, but they should be kept in mind when interpreting the results.

\subsection{Emerging trends and future directions}

Recent surveys and research point to several trends that go beyond the simple SARIMA vs.\ LSTM comparison here:

\begin{itemize}
    \item \textbf{Graph-based and topology-aware models.} GNNs and spatio-temporal graph models are increasingly used to exploit network structure directly.\cite{wu2020gnnsurvey,sun2025cchebnet}
    \item \textbf{Data-centric design.} Handling missing counters, noisy measurements, and changes in measurement methodology is as important as the model architecture.\cite{hu2019dclstm,ferreira2022forecasting}
    \item \textbf{End-to-end integration.} There is growing interest in models that are tightly integrated with routing, admission control, and other control-plane decisions.
\end{itemize}

Natural next steps for this project include using real backbone traces instead of synthetic traffic, incorporating graph neural networks to exploit topology directly, forecasting multiple steps ahead, or experimenting with different safety margins and cost models. The current pipeline is deliberately modular so that those ideas can be plugged in without rewriting everything from scratch.

%=========================================================
% References
%=========================================================

\begin{thebibliography}{99}

\bibitem{papagiannaki2003long}
K.~Papagiannaki, N.~Taft, Z.-L. Zhang, and C.~Diot, ``Long-term forecasting of {Internet} backbone traffic,'' \emph{IEEE Transactions on Neural Networks}, vol.~16, no.~5, pp.~1110--1124, Sep. 2005.

\bibitem{box2015time}
G.~E.~P. Box, G.~M. Jenkins, G.~C. Reinsel, and G.~M. Ljung, \emph{Time Series Analysis: Forecasting and Control}, 5th~ed. Hoboken, NJ, USA: Wiley, 2015.

\bibitem{hochreiter1997lstm}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' \emph{Neural Computation}, vol.~9, no.~8, pp.~1735--1780, 1997.

\bibitem{wu2020gnnsurvey}
Z.~Wu, S.~Pan, F.~Chen, G.~Long, C.~Zhang, and P.~S. Yu, ``A comprehensive survey on graph neural networks,'' \emph{IEEE Transactions on Neural Networks and Learning Systems}, vol.~32, no.~1, pp.~4--24, Jan. 2021.

\bibitem{abbasi2021ntma}
M.~Abbasi, A.~Shahraki, and A.~Taherkordi, ``Deep learning for network traffic monitoring and analysis ({NTMA}): A survey,'' \emph{Computer Communications}, vol.~170, pp.~19--41, Mar. 2021.

\bibitem{chen2021survey}
A.~Chen, J.~Law, and A.~L. Anpalagan, ``A survey on traffic prediction techniques using artificial intelligence for communication networks,'' \emph{Telecom}, vol.~2, no.~4, pp.~518--535, Dec. 2021.

\bibitem{ferreira2022forecasting}
G.~O. Ferreira, C.~Ravazzi, F.~Dabbene, G.~C. Calafiore, and M.~Fiore, ``Forecasting network traffic: A survey and tutorial with open-source comparative evaluation,'' \emph{IEEE Access}, vol.~11, pp.~6018--6044, 2023.

\bibitem{nie2016traffic}
L.~Nie, D.~Jiang, L.~Guo, and S.~Yu, ``Traffic matrix prediction and estimation based on deep learning in large-scale {IP} backbone networks,'' \emph{Journal of Network and Computer Applications}, vol.~76, pp.~16--26, Oct. 2016.

\bibitem{he2017tmdldeep}
D.~Aloraifan, I.~Ahmad, and E.~Alrashed, ``Deep learning based network traffic matrix prediction,'' \emph{International Journal of Intelligent Networks}, vol.~2, pp.~46--56, 2021.

\bibitem{hu2019dclstm}
V.~A. Le, P.-L. Nguyen, and Y.~Ji, ``Deep convolutional {LSTM} network-based traffic matrix prediction with partial information,'' in \emph{Proceedings of the IFIP/IEEE International Symposium on Integrated Network Management (IM)}, Arlington, VA, USA, Apr. 2019, pp.~261--269.

\bibitem{polimi2020dltpno}
S.~Troia, R.~Alvizu, Y.~Zhou, G.~Maier, and A.~Pattavina, ``Deep learning-based traffic prediction for network optimization,'' in \emph{Proceedings of the 20th International Conference on Transparent Optical Networks (ICTON)}, Bucharest, Romania, Jul. 2018, pp.~1--4.

\bibitem{zhang2021deeptm}
J.~Lai, Z.~Chen, J.~Zhu, W.~Ma, L.~Gan, S.~Xie, and G.~Li, ``Deep learning based traffic prediction method for digital twin network,'' \emph{Cognitive Computation}, vol.~15, pp.~1748--1766, 2023.

\bibitem{zhao2023nnotm}
J.~Zhao, H.~Li, and F.~Liu, ``Neural network-based traffic matrix prediction incorporating inter-flow correlations for optical network-on-chip ({ONoC}),'' in \emph{Proceedings of the International Joint Conference on Neural Networks (IJCNN)}, Gold Coast, Australia, Jun. 2023, pp.~1--8.

\bibitem{sun2025cchebnet}
Q.~Sun, R.~Tian, X.~Jia, Q.~Li, and L.~Sun, ``ChebNet traffic flow prediction model based on non-local spatio-temporal correlation matrix,'' \emph{Networks and Spatial Economics}, vol.~25, pp.~935--956, 2025.

\bibitem{zhang2024images}
Z.~Zhang, D.~Shi, Y.~Zhang, and Y.~Sun, ``Network traffic prediction by learning time series as images,'' \emph{Engineering Science and Technology, an International Journal}, vol.~55, p.~101754, 2024.

\end{thebibliography}

\end{document}
